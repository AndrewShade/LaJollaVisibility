{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabf08a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7c966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae52c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "class OceanDataGenerator:\n",
    "    \"\"\"\n",
    "    Aggregates oceanographic and meteorological data to build a dataset for \n",
    "    underwater visibility prediction.\n",
    "\n",
    "    Data sources include CDIP (buoys), NOAA (wind/tide), OpenWeatherMap (rain),\n",
    "    and parsed dive reports.\n",
    "\n",
    "    Attributes:\n",
    "        buoy_id (str): CDIP buoy station ID.\n",
    "        noaa_wind_station (str): NOAA station ID for wind and water level.\n",
    "        owm_api_key (str): OpenWeatherMap API key.\n",
    "        lat (float): Latitude for weather data.\n",
    "        lon (float): Longitude for weather data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.buoy_id = \"201\" # SCRIPPS NEARSHORE, CA\n",
    "        self.buoy_features = [\n",
    "            'waveHs', 'waveTp', 'waveTa', 'waveDp', \n",
    "            'wavePeakPSD', 'sstSeaSurfaceTemperature'\n",
    "        ]\n",
    "        self.noaa_wind_station = \"9410230\" # Scripps Pier\n",
    "        \n",
    "        # Securely load API key from .env file\n",
    "        self.owm_api_key = os.getenv(\"OWM_API_KEY\")\n",
    "        \n",
    "        self.lat, self.lon = 32.8328, -117.2713\n",
    "        self.headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        \n",
    "        # Classification Config\n",
    "        self.viz_bins = [0, 10, 15, 25, 200]\n",
    "        self.viz_labels = [0, 1, 2, 3] # 0: Poor, 1: Fair, 2: Good, 3: Excellent\n",
    "\n",
    "    def _apply_circular_transform(self, df, column):\n",
    "        \"\"\"\n",
    "        Decomposes a directional column (degrees) into sine and cosine components.\n",
    "        \n",
    "        This prevents machine learning models from misinterpreting the discontinuity \n",
    "        between 359 and 0 degrees.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): The dataframe containing the column.\n",
    "            column (str): The name of the column containing degrees (0-360).\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The dataframe with added sine and cosine columns.\n",
    "        \"\"\"\n",
    "        rads = np.deg2rad(df[column])\n",
    "        df[f'{column}_sine'] = np.sin(rads)\n",
    "        df[f'{column}_cos'] = np.cos(rads)\n",
    "        return df\n",
    "\n",
    "    def fetch_buoy_data(self, days=650):\n",
    "        \"\"\"\n",
    "        Fetches and aggregates wave and sea surface temperature data from CDIP THREDDS servers.\n",
    "\n",
    "        Handles both historic (archived) and realtime netCDF files.\n",
    "\n",
    "        Args:\n",
    "            days (int): Number of days of history to fetch.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Daily aggregated wave and temperature data.\n",
    "        \"\"\"\n",
    "        base_url = \"http://thredds.cdip.ucsd.edu/thredds/dodsC/cdip\"\n",
    "        urls = [\n",
    "            f\"{base_url}/archive/{self.buoy_id}p1/{self.buoy_id}p1_historic.nc\",\n",
    "            f\"{base_url}/realtime/{self.buoy_id}p1_rt.nc\"\n",
    "        ]\n",
    "        \n",
    "        start_date = (pd.Timestamp.now() - pd.Timedelta(days=days)).strftime('%Y-%m-%d')\n",
    "        all_wave_aggs, all_sst_aggs = [], []\n",
    "\n",
    "        for url in urls:\n",
    "            try:\n",
    "                ds = xr.open_dataset(url, engine='pydap')\n",
    "                ds_subset = ds[self.buoy_features].sel(waveTime=slice(start_date, None))\n",
    "                \n",
    "                df_wave = ds_subset.drop_dims('sstTime').to_dataframe()\n",
    "                df_wave = self._apply_circular_transform(df_wave, 'waveDp')\n",
    "                \n",
    "                # Feature engineering before daily aggregation\n",
    "                df_wave['wave_steepness'] = df_wave['waveHs'] / df_wave['waveTp']\n",
    "                df_wave['swell_energy'] = (df_wave['waveHs']**2) * df_wave['waveTp']\n",
    "                \n",
    "                wave_agg = df_wave.resample('D').agg({\n",
    "                    'waveHs': ['max', 'mean'], \n",
    "                    'waveTp': 'mean',\n",
    "                    'wave_steepness': 'mean',\n",
    "                    'swell_energy': 'mean',\n",
    "                    'waveDp_sine': 'mean', \n",
    "                    'waveDp_cos': 'mean', \n",
    "                    'wavePeakPSD': 'max'\n",
    "                })\n",
    "                # Flatten MultiIndex columns (e.g., ('waveHs', 'max') -> 'waveHs_max')\n",
    "                wave_agg.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in wave_agg.columns.values]\n",
    "                all_wave_aggs.append(wave_agg)\n",
    "\n",
    "                try:\n",
    "                    df_sst = ds_subset.drop_dims('waveTime').to_dataframe()\n",
    "                    all_sst_aggs.append(df_sst.resample('D').agg({'sstSeaSurfaceTemperature': 'mean'}))\n",
    "                except: pass\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {url}: {e}\")\n",
    "\n",
    "        final_wave = pd.concat(all_wave_aggs).sort_index()\n",
    "        final_wave = final_wave[~final_wave.index.duplicated(keep='last')]\n",
    "        final_sst = pd.concat(all_sst_aggs).sort_index()\n",
    "        final_sst = final_sst[~final_sst.index.duplicated(keep='last')]\n",
    "\n",
    "        return final_wave.join(final_sst, how='inner')\n",
    "\n",
    "    def fetch_wind_data(self, days=650):\n",
    "        \"\"\"\n",
    "        Fetches wind speed, gust, and direction from NOAA API in 30-day chunks.\n",
    "        \n",
    "        Args:\n",
    "            days (int): Number of days of history to fetch.\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: Daily aggregated wind vectors and speeds.\n",
    "        \"\"\"\n",
    "        end_date = pd.Timestamp.now().normalize()\n",
    "        start_date = end_date - pd.Timedelta(days=days)\n",
    "        all_chunks, curr = [], start_date\n",
    "        \n",
    "        while curr < end_date:\n",
    "            curr_end = min(curr + pd.Timedelta(days=30), end_date)\n",
    "            url = (f\"https://api.tidesandcurrents.noaa.gov/api/prod/datagetter?\"\n",
    "                   f\"begin_date={curr.strftime('%Y%m%d')}&end_date={curr_end.strftime('%Y%m%d')}&\"\n",
    "                   f\"station={self.noaa_wind_station}&product=wind&units=metric&time_zone=lst_ldt&format=json\")\n",
    "            try:\n",
    "                res = requests.get(url, headers=self.headers, timeout=15).json()\n",
    "                if 'data' in res: all_chunks.append(pd.DataFrame(res['data']))\n",
    "            except: pass\n",
    "            # Advance slightly to avoid overlap or gap issues in API request\n",
    "            curr = curr_end + pd.Timedelta(minutes=6)\n",
    "\n",
    "        if not all_chunks: return pd.DataFrame()\n",
    "        df = pd.concat(all_chunks).drop_duplicates('t')\n",
    "        df['t'] = pd.to_datetime(df['t'])\n",
    "        df.set_index('t', inplace=True)\n",
    "        for col in ['s', 'g', 'd']: df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        \n",
    "        # Vectorize wind direction to calculate correct daily mean\n",
    "        df['wind_x'], df['wind_y'] = np.cos(np.radians(df['d'])), np.sin(np.radians(df['d']))\n",
    "        daily = df.resample('D').agg({'s': 'mean', 'g': 'max', 'wind_x': 'mean', 'wind_y': 'mean'}).rename(columns={'s': 'wind_speed', 'g': 'wind_gust'})\n",
    "        daily['wind_dir_mean'] = np.degrees(np.arctan2(daily['wind_y'], daily['wind_x'])) % 360\n",
    "        return daily.drop(columns=['wind_x', 'wind_y'])\n",
    "    \n",
    "    def fetch_tide_data(self, days=650):\n",
    "        \"\"\"\n",
    "        Fetches daily tidal max height and mean water level from NOAA.\n",
    "\n",
    "        Args:\n",
    "            days (int): Number of days of history to fetch.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Daily tide metrics.\n",
    "        \"\"\"\n",
    "        end_date = pd.Timestamp.now().normalize()\n",
    "        start_date = end_date - pd.Timedelta(days=days)\n",
    "        all_chunks, curr = [], start_date\n",
    "        \n",
    "        while curr < end_date:\n",
    "            curr_end = min(curr + pd.Timedelta(days=30), end_date)\n",
    "            url = (f\"https://api.tidesandcurrents.noaa.gov/api/prod/datagetter?\"\n",
    "                   f\"begin_date={curr.strftime('%Y%m%d')}&end_date={curr_end.strftime('%Y%m%d')}&\"\n",
    "                   f\"station={self.noaa_wind_station}&product=water_level&datum=mllw&units=metric&time_zone=lst_ldt&format=json\")\n",
    "            try:\n",
    "                res = requests.get(url, headers=self.headers, timeout=15).json()\n",
    "                if 'data' in res: all_chunks.append(pd.DataFrame(res['data']))\n",
    "            except: pass\n",
    "            curr = curr_end + pd.Timedelta(minutes=6)\n",
    "\n",
    "        if not all_chunks: return pd.DataFrame()\n",
    "        df = pd.concat(all_chunks)\n",
    "        df['t'] = pd.to_datetime(df['t'])\n",
    "        df.set_index('t', inplace=True)\n",
    "        df['v'] = pd.to_numeric(df['v'], errors='coerce')\n",
    "        \n",
    "        daily = df.resample('D').agg({'v': ['max', 'mean']})\n",
    "        daily.columns = ['tide_max', 'tide_mean']\n",
    "        return daily\n",
    "\n",
    "    def fetch_rain_data(self, days=365):\n",
    "        \"\"\"\n",
    "        Fetches daily rain summary and calculates a 72-hour weighted accumulation.\n",
    "\n",
    "        Args:\n",
    "            days (int): Number of days of history to fetch.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Daily rain data including a weighted trailing metric.\n",
    "        \"\"\"\n",
    "        rain_data = []\n",
    "        end_date = pd.Timestamp.now().normalize()\n",
    "        curr = end_date - pd.Timedelta(days=days)\n",
    "        \n",
    "        while curr <= end_date:\n",
    "            url = f\"https://api.openweathermap.org/data/3.0/onecall/day_summary?lat={self.lat}&lon={self.lon}&date={curr.strftime('%Y-%m-%d')}&appid={self.owm_api_key}&units=metric\"\n",
    "            try:\n",
    "                r = requests.get(url).json()\n",
    "                rain_data.append({'time': curr, 'rain_mm': r.get('precipitation', {}).get('total', 0)})\n",
    "            except: pass\n",
    "            curr += pd.Timedelta(days=1)\n",
    "        \n",
    "        df = pd.DataFrame(rain_data).set_index('time')\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        \n",
    "        # Apply a decay factor: Today + (Yesterday * 0.6) + (2 Days Ago * 0.3)\n",
    "        # This models runoff lag, where past rain still affects current water clarity.\n",
    "        df['rain_72h_weighted_mm'] = (df['rain_mm'] + (df['rain_mm'].shift(1) * 0.6) + (df['rain_mm'].shift(2) * 0.3)).fillna(0)\n",
    "        return df\n",
    "\n",
    "    def scrape_visibility_labels(self, total_pages=27):\n",
    "        \"\"\"\n",
    "        Scrapes dive reports from a blog to extract visibility distance labels.\n",
    "\n",
    "        Args:\n",
    "            total_pages (int): Number of pagination pages to scrape.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Dates mapped to visibility distance in feet.\n",
    "        \"\"\"\n",
    "        base_url = \"https://justgetwet.com/blogs/dive-reports-and-conditions?page=\"\n",
    "        all_reports = []\n",
    "        for page_num in range(1, total_pages + 1):\n",
    "            try:\n",
    "                res = requests.get(f\"{base_url}{page_num}\", headers=self.headers, timeout=10)\n",
    "                soup = BeautifulSoup(res.text, 'html.parser')\n",
    "                for art in soup.find_all('div', class_='article__grid-meta'):\n",
    "                    date_tag, excerpt = art.find('time'), art.find('div', class_='article__excerpt')\n",
    "                    if not date_tag or not excerpt: continue\n",
    "                    \n",
    "                    # Regex to find 'Viz: 10-15'' or 'Vis: 5m' patterns\n",
    "                    viz_raw = re.search(r'(?:Viz|Vis|Visibility):\\s*([\\d\\-\\+m\\s\\']+)', excerpt.get_text(), re.IGNORECASE)\n",
    "                    if viz_raw:\n",
    "                        nums = re.findall(r'(\\d+)', viz_raw.group(1).lower())\n",
    "                        if not nums: continue\n",
    "                        viz_ft = float(nums[0])\n",
    "                        # Normalize meters to feet if 'm' is present\n",
    "                        if 'm' in viz_raw.group(1).lower(): viz_ft *= 3.28\n",
    "                        all_reports.append({'date': pd.to_datetime(date_tag.get_text().strip()), 'visibility_ft': viz_ft})\n",
    "                time.sleep(0.5)\n",
    "            except: pass\n",
    "        df = pd.DataFrame(all_reports).drop_duplicates('date')\n",
    "        df['date'] = pd.to_datetime(df['date']).dt.normalize()\n",
    "        return df.set_index('date').sort_index()\n",
    "\n",
    "    def run(self, days=650, scrape_pages=27):\n",
    "        \"\"\"\n",
    "        Orchestrates the data pipeline: fetching, joining, and feature engineering.\n",
    "\n",
    "        Args:\n",
    "            days (int): Days of history to fetch for features.\n",
    "            scrape_pages (int): Number of blog pages to scrape for labels.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The final cleaned dataset ready for training.\n",
    "        \"\"\"\n",
    "        df_buoy = self.fetch_buoy_data(days=days)\n",
    "        df_wind = self.fetch_wind_data(days=days)\n",
    "        df_rain = self.fetch_rain_data(days=days)\n",
    "        df_tide = self.fetch_tide_data(days=days)\n",
    "        df_labels = self.scrape_visibility_labels(total_pages=scrape_pages)\n",
    "\n",
    "        # Normalize indices to ensure clean joins\n",
    "        for d in [df_buoy, df_wind, df_rain, df_tide, df_labels]:\n",
    "            if not d.empty:\n",
    "                d.index = pd.to_datetime(d.index).normalize().tz_localize(None)\n",
    "\n",
    "        # Store intermediates for debugging/auditing\n",
    "        self.df_buoy = df_buoy.copy() \n",
    "        self.df_wind = df_wind.copy()\n",
    "        self.df_rain = df_rain.copy()\n",
    "        self.df_tide = df_tide.copy()\n",
    "        self.df_labels = df_labels.copy()\n",
    "        \n",
    "        audit_dict = {\n",
    "            'df_buoy': self.df_buoy, 'df_wind': self.df_wind,\n",
    "            'df_rain': self.df_rain, 'df_labels': self.df_labels,\n",
    "            'df_tide': self.df_tide\n",
    "        }\n",
    "        for k, v in audit_dict.items():\n",
    "            print(f\"{k} min: {v.index.min()} {k} max: {v.index.max()}\")\n",
    "        \n",
    "        if 'sstSeaSurfaceTemperature' in df_buoy.columns:\n",
    "            df_buoy['sst_diff_24h'] = df_buoy['sstSeaSurfaceTemperature'].diff(1)\n",
    "            df_buoy['sst_diff_48h'] = df_buoy['sstSeaSurfaceTemperature'].diff(2)\n",
    "\n",
    "        final_df = df_buoy.join(df_wind, how='inner').join(df_rain, how='inner')\\\n",
    "                          .join(df_tide, how='inner').join(df_labels, how='inner')\n",
    "\n",
    "        # Add Seasonality features (Cyclical encoding of day-of-year)\n",
    "        day_of_year = pd.to_datetime(final_df.index).dayofyear\n",
    "        final_df['season_sine'] = np.sin(2 * np.pi * day_of_year / 365.25)\n",
    "        final_df['season_cos'] = np.cos(2 * np.pi * day_of_year / 365.25)\n",
    "\n",
    "        # Generate lag features (1-3 days prior) for time-series context\n",
    "        cols_to_lag = ['waveHs_mean', 'waveHs_max', 'wind_speed', 'wavePeakPSD_max', 'rain_72h_weighted_mm']\n",
    "        for col in cols_to_lag:\n",
    "            if col in final_df.columns:\n",
    "                final_df[f'{col}_lag1'] = final_df[col].shift(1)\n",
    "                final_df[f'{col}_lag2'] = final_df[col].shift(2)\n",
    "                final_df[f'{col}_lag3'] = final_df[col].shift(3)\n",
    "\n",
    "        if 'waveHs_mean' in final_df.columns:\n",
    "            final_df['swell_trend_3d'] = final_df['waveHs_mean'].diff(periods=3)\n",
    "            \n",
    "        print(f\"final_df Pre NA Drop {final_df.shape} Post NA Drop: {final_df.dropna().shape}\")\n",
    "        self.data = final_df.dropna()\n",
    "        return self.data\n",
    "\n",
    "    def save_data(self, df, path=\"training_data.parquet\", as_classification=False):\n",
    "        \"\"\"\n",
    "        Saves the dataframe to parquet, optionally binning the target for classification.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): The dataframe to save.\n",
    "            path (str): Output filepath.\n",
    "            as_classification (bool): If True, bins visibility into classes (0-3).\n",
    "                                      If False, keeps visibility as continuous regression target.\n",
    "        \"\"\"\n",
    "        export_df = df.copy()\n",
    "        if as_classification:\n",
    "            export_df['target'] = pd.cut(\n",
    "                export_df['visibility_ft'], \n",
    "                bins=self.viz_bins, \n",
    "                labels=self.viz_labels,\n",
    "                include_lowest=True\n",
    "            ).astype(int)\n",
    "            export_df.drop(columns=['visibility_ft'], inplace=True)\n",
    "        else:\n",
    "            export_df.rename(columns={'visibility_ft': 'target'}, inplace=True)\n",
    "            \n",
    "        export_df.to_parquet(path, engine=\"fastparquet\")\n",
    "        print(f\"Success. Parquet saved to {path} with {len(export_df)} records.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3523c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andrew Shade\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pydap\\handlers\\dap.py:143: UserWarning: PyDAP was unable to determine the DAP protocol defaulting to DAP2. DAP2 is consider legacy and may result in slower responses. \n",
      "Consider replacing `http` in your `url` with either `dap2` or `dap4` to specify the DAP protocol (e.g. `dap2://<data_url>` or `dap4://<data_url>`).  For more \n",
      "information, go to https://www.opendap.org/faq-page.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Andrew Shade\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pydap\\handlers\\dap.py:143: UserWarning: PyDAP was unable to determine the DAP protocol defaulting to DAP2. DAP2 is consider legacy and may result in slower responses. \n",
      "Consider replacing `http` in your `url` with either `dap2` or `dap4` to specify the DAP protocol (e.g. `dap2://<data_url>` or `dap4://<data_url>`).  For more \n",
      "information, go to https://www.opendap.org/faq-page.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_buoy min: 2024-03-18 00:00:00 df_buoy max: 2026-02-14 00:00:00\n",
      "df_wind min: 2024-03-18 00:00:00 df_wind max: 2026-02-13 00:00:00\n",
      "df_rain min: 2024-03-18 00:00:00 df_rain max: 2026-02-13 00:00:00\n",
      "df_labels min: 2024-03-16 00:00:00 df_labels max: 2026-02-10 00:00:00\n",
      "df_tide min: 2024-03-18 00:00:00 df_tide max: 2026-02-13 00:00:00\n",
      "final_df Pre NA Drop (521, 37) Post NA Drop: (518, 37)\n"
     ]
    }
   ],
   "source": [
    "gen = OceanDataGenerator()\n",
    "df = gen.run(697,27)\n",
    "gen.save_data(df,path=\"visibility_data_class.parquet\",as_classification=True)\n",
    "gen.save_data(df,path=\"visibility_data_reg.parquet\",as_classification=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
